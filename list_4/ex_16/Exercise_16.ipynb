{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "# from keras import layers\n",
    "# from keras.utils import to_categorical\n",
    "# from keras.models import Sequential, Model, load_model\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "# from keras.preprocessing.text import Tokenizer\n",
    "# from keras.optimizers import Adam, SGD\n",
    "\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "np.random.seed(99)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, plot_name):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{plot_name}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "test_set = fetch_20newsgroups(subset='test', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "vectorizer = StemmedCountVectorizer(stop_words='english')\n",
    "# vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer.fit(test_set.data)\n",
    "\n",
    "X_train = vectorizer.transform(train_set.data)\n",
    "X_test  = vectorizer.transform(test_set.data)\n",
    "\n",
    "num_classes = np.max(train_set[\"target\"]) + 1\n",
    "\n",
    "y_train = to_categorical(train_set.target, num_classes)\n",
    "y_test = to_categorical(test_set.target, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplest Neural Network with Simple Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_simple_network(parameters):\n",
    "    print(f'Performing training on the following parameters {parameters}')\n",
    "    \n",
    "    input_dim = X_train.shape[1]\n",
    "    output_dim = y_train.shape[1]\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.Dense(\n",
    "        parameters[\"dense_layer_neurons\"], \n",
    "        input_dim=input_dim,\n",
    "        activation='relu'\n",
    "    ))\n",
    "    \n",
    "    if parameters[\"use_deeper_network\"]:\n",
    "        model.add(layers.Dense(\n",
    "            32, \n",
    "            activation='relu'\n",
    "        ))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(output_dim, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=parameters[\"optimizer\"],\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=parameters[\"epochs\"],\n",
    "        verbose=2,\n",
    "        validation_data=(X_test, y_test),\n",
    "        batch_size=parameters[\"batch_size\"],\n",
    "    )\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    return {'loss': -val_accuracy, 'status': STATUS_OK, 'model': model, 'history': history} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"dense_layer_neurons\": hp.choice(\"dense_layer_neurons\", [64, 96, 128]),\n",
    "    \"optimizer\": hp.choice(\"optimizer\", [Adam(), SGD()]),\n",
    "    \"batch_size\": hp.choice(\"batch_size\", [10, 32]),\n",
    "    \"epochs\": hp.choice(\"epochs\", [25, 50]),\n",
    "    \"use_deeper_network\": hp.choice(\"use_deeper_network\", [True, False])\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "max_evals = 3 * 2 * 2 * 2 * 2\n",
    "\n",
    "best = fmin(\n",
    "    fn=train_simple_network,\n",
    "    space=parameters,\n",
    "    algo=tpe.suggest,\n",
    "    trials=trials,\n",
    "    max_evals=max_evals\n",
    ")\n",
    "\n",
    "best_model = trials.best_trial['result']['model']\n",
    "history = trials.best_trial['result']['history']\n",
    "\n",
    "best_model.save('simple_model.hd5')\n",
    "plot_history(history, \"simple_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplest Neural Network with More Complex Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_network_with_preprocessing(parameters):\n",
    "    embedding_dim = 50\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                               output_dim=embedding_dim, \n",
    "                               input_length=maxlen))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(parameters[\"dense_layer_neurons\"], activation='relu'))\n",
    "    model.add(layers.Dense(output_dim, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=parameters[\"optimizer\"],\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, \n",
    "        y_train,\n",
    "        epochs=parameters[\"epochs\"],\n",
    "        validation_data=(X_test, y_test),\n",
    "        batch_size=parameters[\"batch_size\"]\n",
    "    )\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    return {'loss': -val_accuracy, 'status': STATUS_OK, 'model': model, 'history': history}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=3000)\n",
    "tokenizer.fit_on_texts(train_set.data)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(train_set.data)\n",
    "X_test = tokenizer.texts_to_sequences(test_set.data)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "maxlen = 300\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "parameters = {\n",
    "    \"dense_layer_neurons\": hp.choice(\"dense_layer_neurons\", [32, 64, 128]),\n",
    "    \"optimizer\": hp.choice(\"optimizer\", [Adam(), SGD()]),\n",
    "    \"batch_size\": hp.choice(\"batch_size\", [10, 32]),\n",
    "    \"epochs\": hp.choice(\"epochs\", [25, 50]),\n",
    "}\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = y_train.shape[1]\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "max_evals = 3 * 2 * 2 * 2\n",
    "\n",
    "best = fmin(\n",
    "    fn=train_network_with_preprocessing,\n",
    "    space=parameters,\n",
    "    algo=tpe.suggest,\n",
    "    trials=trials,\n",
    "    max_evals=max_evals\n",
    ")\n",
    "\n",
    "best_model = trials.best_trial['result']['model']\n",
    "history = trials.best_trial['result']['history']\n",
    "\n",
    "best_model.save('simple_model_preprocessing.hd5')\n",
    "plot_history(history, \"simple_model_preprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overkill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "TODO:\n",
    "\n",
    "1) fast text\n",
    "\n",
    "2) biLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # coding=utf-8\n",
    "\n",
    "# import numpy as np\n",
    "# from keras.callbacks import EarlyStopping\n",
    "# from keras.datasets import imdb\n",
    "# from keras.preprocessing import sequence\n",
    "\n",
    "# # from fast_text import FastText\n",
    "\n",
    "# from keras import Input, Model\n",
    "# from keras.layers import Embedding, GlobalAveragePooling1D, Dense\n",
    "\n",
    "\n",
    "# class FastText(object):\n",
    "#     def __init__(self, maxlen, max_features, embedding_dims,\n",
    "#                  class_num=1,\n",
    "#                  last_activation='sigmoid'):\n",
    "#         self.maxlen = maxlen\n",
    "#         self.max_features = max_features\n",
    "#         self.embedding_dims = embedding_dims\n",
    "#         self.class_num = class_num\n",
    "#         self.last_activation = last_activation\n",
    "\n",
    "#     def get_model(self):\n",
    "#         input = Input((self.maxlen,))\n",
    "\n",
    "#         embedding = Embedding(self.max_features, self.embedding_dims, input_length=self.maxlen)(input)\n",
    "#         x = GlobalAveragePooling1D()(embedding)\n",
    "\n",
    "#         output = Dense(self.class_num, activation=self.last_activation)(x)\n",
    "#         model = Model(inputs=input, outputs=output)\n",
    "#         return model\n",
    "    \n",
    "# def create_ngram_set(input_list, ngram_value=2):\n",
    "#     \"\"\"\n",
    "#     Extract a set of n-grams from a list of integers.\n",
    "#     # >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n",
    "#     {(4, 9), (4, 1), (1, 4), (9, 4)}\n",
    "#     # >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n",
    "#     [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n",
    "#     \"\"\"\n",
    "#     return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n",
    "\n",
    "\n",
    "# def add_ngram(sequences, token_indice, ngram_range=2):\n",
    "#     \"\"\"\n",
    "#     Augment the input list of list (sequences) by appending n-grams values.\n",
    "#     Example: adding bi-gram\n",
    "#     # >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "#     # >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n",
    "#     # >>> add_ngram(sequences, token_indice, ngram_range=2)\n",
    "#     [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n",
    "#     Example: adding tri-gram\n",
    "#     # >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
    "#     # >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n",
    "#     # >>> add_ngram(sequences, token_indice, ngram_range=3)\n",
    "#     [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42, 2018]]\n",
    "#     \"\"\"\n",
    "#     new_sequences = []\n",
    "#     for input_list in sequences:\n",
    "#         new_list = input_list[:]\n",
    "#         for ngram_value in range(2, ngram_range + 1):\n",
    "#             for i in range(len(new_list) - ngram_value + 1):\n",
    "#                 ngram = tuple(new_list[i:i + ngram_value])\n",
    "#                 if ngram in token_indice:\n",
    "#                     new_list.append(token_indice[ngram])\n",
    "#         new_sequences.append(new_list)\n",
    "\n",
    "#     return new_sequences\n",
    "\n",
    "\n",
    "# # Set parameters:\n",
    "# # ngram_range = 2 will add bi-grams features\n",
    "# ngram_range = 1\n",
    "# max_features = 5000\n",
    "# maxlen = 400\n",
    "# batch_size = 32\n",
    "# embedding_dims = 50\n",
    "# epochs = 10\n",
    "# print('Loading data...')\n",
    "# (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(x_train), 'train sequences')\n",
    "# print(len(x_test), 'test sequences')\n",
    "# print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\n",
    "# print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\n",
    "\n",
    "# if ngram_range > 1:\n",
    "#     print('Adding {}-gram features'.format(ngram_range))\n",
    "#     # Create set of unique n-gram from the training set.\n",
    "#     ngram_set = set()\n",
    "#     for input_list in x_train:\n",
    "#         for i in range(2, ngram_range + 1):\n",
    "#             set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n",
    "#             ngram_set.update(set_of_ngram)\n",
    "\n",
    "#     # Dictionary mapping n-gram token to a unique integer.\n",
    "#     # Integer values are greater than max_features in order\n",
    "#     # to avoid collision with existing features.\n",
    "#     start_index = max_features + 1\n",
    "#     token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n",
    "#     indice_token = {token_indice[k]: k for k in token_indice}\n",
    "\n",
    "#     # max_features is the highest integer that could be found in the dataset.\n",
    "#     max_features = np.max(list(indice_token.keys())) + 1\n",
    "\n",
    "#     # Augmenting x_train and x_test with n-grams features\n",
    "#     x_train = add_ngram(x_train, token_indice, ngram_range)\n",
    "#     x_test = add_ngram(x_test, token_indice, ngram_range)\n",
    "#     print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\n",
    "#     print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\n",
    "\n",
    "# print('Pad sequences (samples x time)...')\n",
    "# x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "# x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "# print('x_train shape:', x_train.shape)\n",
    "# print('x_test shape:', x_test.shape)\n",
    "\n",
    "# print('Build model...')\n",
    "# model = FastText(maxlen, max_features, embedding_dims).get_model()\n",
    "# model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# print('Train...')\n",
    "# early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "# model.fit(x_train, y_train,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           callbacks=[early_stopping],\n",
    "#           validation_data=(x_test, y_test))\n",
    "\n",
    "# print('Test...')\n",
    "# result = model.predict(x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
